---
title: Interactive Dance Projects 
date: "2019-06-01"
thumbnail: ./dance.png
figcaption: Screenshot of an interactive dance performance with a liquid particle simulation
description: I developed a series of interactive music system that use motion capture to translate movement into sound and music. While first developed for university assignments, I managed to present this work at different venues often in collaboration with professional dancers.
---

I noticed that a lot of second-hand electronic shops in London sell **Microsoft Kinects** sparking the idea of using motion capture (MoCap) for electronic music performance. **Imagine how much more interesting live performances would be if DJs could use their whole body** rather than being hunched over a laptop or turntables. Originally developed for the *Xbox* gaming console, the *Kinect* is an affordable MoCap tool that can be connected to a MacBook via Software Development Kits (SDKs).

For my first experiment, I created a project <a rel="noopener noreferrer" target="_blank" href="https://www.ableton.com/">Ableton Live</a> where audio effects and song arrangement can be controlled by changing one's arms and body positions. Here is a video of life performance:

`vimeo:https://player.vimeo.com/video/218237180`

I developed the next iteration together with a professional dancer. The idea was to map their body onto a digital cube mimicking their position and triggering different tones depending on the movement enabling them to have agency over melody, tempo and intensity. Have a look at the result: 

`vimeo:https://player.vimeo.com/video/221183260`

This setup might give more control, but it makes it really difficult to produce music that does not sound overly random. For the next iteration, I decided to adopt a hybrid approach that combines <a rel="noopener noreferrer" target="_blank" href="https://toplap.org/about/">live coding</a> with interactive dance. The result was a setup that includes a programmer and a dancer as performers. The performance in the video below was part of the <a rel="noopener noreferrer" target="_blank" href="https://www.wesa.kr/Wesa">WeSa Festival</a>:

`vimeo:https://player.vimeo.com/video/255499739`

After starting my PhD in 2018, I worked as an intern for the tech-startup <a rel="noopener noreferrer" target="_blank" href="https://www.mxx.ai/">MXX</a>. Their CEO used to be a dancer, really enjoyed my interactive dance work and asked me to combine it with their technology. MXX develops tools that make it easy to edit music by adjusting high-level parameters like *intensity*, *excitement* or *energy*. I used the *Kinect* to transport a dancer into a liquid particle simulation that measures the movement intensity from which the MXX technology creates a composition.  The performance in the video below features <a rel="noopener noreferrer" target="_blank" href="https://www.thrivedance.co.uk/">Sarah Poekert</a>, a choreographer with whom I have an ongoing collaboration.

`youtube:https://youtu.be/fYInCkaOZO8`