---
title: Interactive Dance Projects 
date: "2019-06-01"
thumbnail: ./dance.png
figcaption: Screenshot of an interactive dance performance with a liquid particle simulation
description: During my Masters, I developed a series of interactive music system that use motion capture to translate movement into sound and music. While first developed for university assignments, I managed to present this work at different venues in London and internationally often in collaboration with professional dancers.
---

When moving to London, I noticed that a lot of second-hand electronic shops offer **Microsoft Kinects** for really low prices. The *Kinect* was originally developed for the gaming console Xbox but never really took off beyond the first novelty wave. Luckily, some tech-savvy people developed Software Development Kits (SDKs) that let you connect the *Kinect* to you MacBook and make use of its motion capture capabilities. I thought that motion capture could be a good means to make electronic music performances more interesting. **Imagine how much more interesting live performances would be if DJs could use their whole body to create their sets** rather than being hunched over a laptop or turntables. 

So I got a *Kinect* and a developed my first project that let me use my arms and body to control an electronic music composition that I composed in <a rel="noopener noreferrer" target="_blank" href="https://www.ableton.com/">Ableton Live</a>. I performed this setup at *The Amersham Arms* in New Cross, London which is a great venue to present experimental stuff like this. Here is a video from the performance: 

`vimeo:https://player.vimeo.com/video/218237180`

I quite enjoy this performance, but I didn't like that it basically only let me re-arrange pre-composed snippets. So I started exploring ways to map movement more directly to music that could enable a user to create new melodies and compositions. The next development aimed to capture the whole body of a dancer and translate their movement into music during a performance. Have a look a this choreography performed by my then girlfriend (now wife) at <a rel="noopener noreferrer" target="_blank" href="https://www.gold.ac.uk/">Goldsmiths University of London</a>.

`vimeo:https://player.vimeo.com/video/221183260`

While this setup can give definitely give more control over structure and composition, it was really difficult to produce something that didn't sound super experimental. If I compare this performance with the first one, I would say that the former is a lot more interesting to listen to. So for the next iteration, I decided to adopt a hybrid approach that combines <a rel="noopener noreferrer" target="_blank" href="https://toplap.org/about/">live coding</a> with interactive dance. The result was a setup that includes a programmer and a dancer as performers. Since I did my Masters at Goldsmiths, a university known for its experimental art projects, this project also came with a whole conceptual backdrop of **digital occultism** and **ASCII art** I presented this piece multiple times together with a choreographer friend (they became kind of famous recently, so I'm careful about naming them...). The performance in the video below was at the <a rel="noopener noreferrer" target="_blank" href="https://www.wesa.kr/Wesa">WeSa Festival</a> in Seoul 2017. The music is still a lot more experimental than in the first video, but it is by design that could easily be changed rather than an unwanted byproduct as in the second video.

`vimeo:https://player.vimeo.com/video/255499739`

When I started my PhD in 2018, my interactive dance projects were revived during an internship with the tech-startup <a rel="noopener noreferrer" target="_blank" href="https://www.mxx.ai/">MXX</a>. Their CEO used to be a dancer, really enjoyed my work and asked me to develop a system that uses their software. MXX develops tools that make it easy to re-arrange compositions simply by adjusting high-level parameters like *intensity*, *excitement* or *energy*. It was originally meant for people to cut music to movie trailers, but I repurposed it to accompany a dance performance. I used the *Kinect* to transport a dancer into a liquid particle simulation that measures the intensity of their movement, sends it to the MXX software which picks a suitable snippet. The performance in the video below features <a rel="noopener noreferrer" target="_blank" href="https://www.thrivedance.co.uk/">Sarah Poekert</a>, a choreographer with whom I have an ongoing collaboration.

`youtube:https://youtu.be/fYInCkaOZO8`

Looking back, it's really cool to see how all of these different projects and performances evolved from a simple idea. As a nice extra, I also managed to hone my technology and programming skills especially with respect to **human-computer interaction**.